{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIg6enKVhhi4"
      },
      "source": [
        "\n",
        "\n",
        "## **COMP6685 Deep Learning Coursework A1**\n",
        "\n",
        "\n",
        "Individual (25% of total mark)\n",
        "\n",
        "\n",
        "**TASK:**\tYou are required to develop a phyton code with appropriate comments and answer questions.\n",
        "\n",
        "**Description**: Create a code using this temlate to train a Convolutional Neural Network (CNN) on the fashion MNIST dataset available at https://keras.io/api/datasets/fashion_mnist/ . \n",
        "\n",
        "Fashion MNIST is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images.\n",
        "\n",
        "The dataset should be imported in the code and one sample image should be visualised before applying the model.\n",
        "\n",
        "Define a CNN and comment the chosen parameters of the network. Apply a regularization method (L1, L2 or L1L2). Divide the dataset into training, validation and test set. Obtain the accuracy on the validation set and plot the final results using the data from the test set. Comment your lines of code appropriately to explain your solution.\n",
        "\n",
        "Enhance the model's performance to obtain the best or optimal validation accuracy. Further questions about final remarks on the results will be answered on the markdown defined in the template.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "Note: This is only a template. You can add more code/text cells if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qysRQbvOm1c4"
      },
      "source": [
        "Import the dataset and divide it into training, validation and test sets. Explain how you obtained the validation set. How did you choose the size of the validation set? **(10 marks)**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV9PO1GBl5_6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "OPTIMIZER = SGD(learning_rate=0.1) # Stochastic gradient descent optimiser\n",
        "\n",
        "# importing of service libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#each 2D image consists of 28x28 values/pixels, which needs to be reshaped in a vector of 784 pixels\n",
        "RESHAPED = 784\n",
        "\n",
        "\n",
        "print('Libraries imported.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#training constants\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCH = 20  \n",
        "N_CLASSES = 10\n",
        "VERBOSE = 1\n",
        "VALIDATION_SPLIT = 0.2\n",
        "OPTIM = RMSprop()\n",
        "\n",
        "print('Main variables initialised.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fashion_MNIST is a set of 60K images 28x28 pixels\n",
        "IMG_CHANNELS = 1\n",
        "IMG_ROWS = 28\n",
        "IMG_COLS = 28\n",
        "\n",
        "print('Image variables initialisation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load dataset\n",
        "(input_X_train, output_y_train), (input_X_test, output_y_test) = fashion_mnist.load_data()\n",
        "print('input_X_train shape:', input_X_train.shape)\n",
        "print(input_X_train.shape[0], 'train samples')\n",
        "print(input_X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert to categorical\n",
        "output_Y_train = utils.to_categorical(output_y_train, N_CLASSES)\n",
        "output_Y_test = utils.to_categorical(output_y_test, N_CLASSES) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# i used a value of 20% for the validation set as this is standard, i also tried 10% but that didnt make much of a change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-xHphsvmG7F"
      },
      "source": [
        "Visualise a random sample image of the dataset. **(10 marks)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTTrIN_nnDWK"
      },
      "outputs": [],
      "source": [
        "# visualisation of the numerical vector and 2D colour plot of the sample fashion_mnist imnage \n",
        "Selected_Image = 1\n",
        "image = input_X_train[Selected_Image]\n",
        "print (\"Sample input image: \" + str(image))\n",
        "plt.imshow(image)\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMhGFcimoUwz"
      },
      "source": [
        "Define your CNN model. Specify the network and training parameters and comment them. **(10 marks)**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# float and normalization\n",
        "input_X_train = input_X_train.astype('float32')\n",
        "input_X_test = input_X_test.astype('float32')\n",
        "input_X_train = input_X_train.astype('float32')\n",
        "input_X_test = input_X_test.astype('float32')\n",
        "input_X_train /= 255\n",
        "input_X_test /= 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urupEGOFooUz"
      },
      "outputs": [],
      "source": [
        "# CNN model definition\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Conv2D(32, kernel_size=3, padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, kernel_size=3, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        " \n",
        "model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(N_CLASSES))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csCYqn-9pDuJ"
      },
      "source": [
        "Train the CNN model. **(10 marks)**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xESPeCWnpJek"
      },
      "outputs": [],
      "source": [
        "# import the regularizers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "N_EPOCH = 20\n",
        "N_HIDDEN = 128\n",
        "P_DROPOUT = 0.3\n",
        "\n",
        "input_X_train = input_X_train.reshape(60000, RESHAPED)\n",
        "input_X_test = input_X_test.reshape(10000, RESHAPED)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Hidden layer 1 with 128 hidden units and ReLu activation function\n",
        "model.add(Dense(N_HIDDEN, activity_regularizer=regularizers.L2(1e-5),input_shape=(RESHAPED,)))\n",
        "model.add(Activation('relu'))          \n",
        "\n",
        "\n",
        "# output layer with 10 units and softmax activation\n",
        "model.add(Dense(N_CLASSES))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "# model compilation\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(input_X_train, output_Y_train, batch_size=BATCH_SIZE, epochs=N_EPOCH, validation_split=VALIDATION_SPLIT,  verbose=VERBOSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFMiFtDCpjMG"
      },
      "source": [
        "Evaluate your model. What is the best/highest validation accuracy your network achieved? How did you obtain this accuracy? **(10 marks)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnCeV6qP6xmT"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(input_X_test, output_Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
        "print(\"\\nTest score/loss:\", score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4GvX1moIqqm0"
      },
      "source": [
        "# with my base model i got a test accuracy of 74% and a test score of 5.5 i knew this wasnt the best accuracy and that i could make it better with the use of regularization. i was able to get an 86% test accuracy and a test score of 3.6. this was done using L2 regularization. this was the best one as using L1 or a combination of L1L2 led to worse test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S48e3kq_qCCi"
      },
      "source": [
        "Plot the final results on the test set and print the accuracy/loss on that set. **(10 marks)**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLLIrvy2psKZ"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "#plt.plot(mo)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ImI7busBrCie"
      },
      "source": [
        "Additional questions:\n",
        "\n",
        "\n",
        "*   Describe whether you found any differences in the network’s accuracy when applying regularisation compared to not applying it. If there were differences, which regularisation did you use? If no differences were found, what could be the reason? **(10 marks)**\n",
        "\n",
        "# applying regularization increased the networks efficency, this is because the total amount of parameters was reduced by 100,000. this also meant that it was easier to train the network and this increased the test accuracy as well as decreasing the test score. Also, it made the process of training much faster, i was able to train the network much faster.\n",
        "\n",
        "*   Write your conclusions about the results achieved with your model on the fashion MNIST dataset and ideas to improve these results/performance further. **(10 marks)**\n",
        "\n",
        "# i couldve used a larger training set, this could help as the CNN would have more variations of the image to go off of. to do this i could use data augmentation. also i couldve added another dropout regularizer after using L2, this is because it would help with overfitting.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcSc_1JiqRA3"
      },
      "source": [
        "Additional remarks:\n",
        "\n",
        "*   Code outline appropriately commented. **(10 marks)**\n",
        "*   Code running without errors. **(10 marks)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
